# Changelog - E-commerce ML Pipeline Setup

## [2025-03-07] - Project Setup and Data Management

### Added
1. Project Structure:
   - Created essential directories for Airflow (dags, logs, plugins, config)
   - Set up data directories (raw, processed)
   - Added utility scripts in src/utils/

2. Docker Configuration:
   - Implemented docker-compose.yaml for Airflow services
   - Created airflow.env for environment variables
   - Successfully deployed Airflow with PostgreSQL backend

3. Data Management:
   - Added RetailRocket dataset to data/raw/
   - Created data sampling utility (src/utils/create_samples.py)
   - Generated 1000-row samples for all dataset files in data/raw/samples/
   - Added AWS integration utilities (src/utils/s3_utils.py)
   - Added AWS credentials configuration script (src/utils/configure_aws.py)

4. Dependencies:
   - Updated requirements.txt with necessary packages:
     - apache-airflow and AWS provider
     - pandas for data processing
     - boto3 for AWS integration
     - psycopg2-binary for PostgreSQL

### Technical Decisions
1. Docker Implementation:
   - Chose Docker for consistent development environment
   - Set up multi-container architecture with PostgreSQL
   - Implemented custom initialization script

2. Data Strategy:
   - Decided to keep sample data in git for development
   - Plan to store full dataset in S3 (pending AWS setup)
   - Implemented .gitignore rules to exclude large data files

3. Development Workflow:
   - Created utility scripts for repeatable operations
   - Set up local development with sample data
   - Prepared AWS integration tools

### Current Status
- Airflow running in Docker
- Sample data generated and stored
- AWS utilities prepared
- AWS S3 setup pending
- Full dataset upload pending

### Next Steps
1. Complete AWS Setup:
   - Create IAM user with S3 access
   - Configure AWS credentials
   - Create S3 bucket
   - Upload full dataset

2. Data Pipeline Development:
   - Create first Airflow DAG
   - Implement data processing logic
   - Set up model training pipeline

### Notes
- All configuration follows industry best practices
- Infrastructure defined as code (IaC)
- Development environment mirrors production setup

## [2025-03-07] - AWS Setup and Dataset Upload

### Added
1. AWS Configuration:
   - Created IAM user with S3 access
   - Implemented AWS credentials configuration script
   - Added region handling in configuration utilities

2. S3 Integration:
   - Created S3 bucket (ecommerce-ml-pipeline-data) in eu-west-1
   - Enhanced s3_utils.py with better region handling
   - Added AWS region detection from credentials

3. Dataset Upload:
   - Successfully uploaded RetailRocket dataset to S3:
     - category_tree.csv (14.4 KB)
     - events.csv (94.2 MB)
     - item_properties_part1.csv (484.3 MB)
     - item_properties_part2.csv (408.9 MB)
   - Organized under 'raw/' prefix in S3

### Technical Decisions
1. AWS Configuration:
   - Used eu-west-1 (Ireland) region for lower latency
   - Implemented proper UTF-8 encoding for credentials
   - Added automatic region detection from credentials

2. Data Organization:
   - Used 'raw/' prefix for unprocessed data
   - Uploaded unzipped CSV files directly
   - Maintained original file structure

### Current Status
- AWS credentials configured
- S3 bucket created and accessible
- Full dataset uploaded to S3
- Local development environment ready

### Next Steps
1. Data Pipeline Development:
   - Create first Airflow DAG
   - Implement data processing logic
   - Set up model training pipeline

## [2025-03-07] - Model Training Pipeline Development

### Added
- Implemented ALS (Alternating Least Squares) model for collaborative filtering
  - Added ALSTrainer class with training, prediction, and model persistence
  - Support for user-item recommendations and similar items
  
- Implemented CatBoost ranking model
  - Added RankingTrainer class with YetiRank loss function
  - Support for group-wise learning and evaluation metrics (NDCG, MAP)
  
- Created model training pipeline DAG with tasks:
  1. load_data: Load processed data from S3
  2. train_als: Train and save ALS model
  3. generate_candidates: Generate candidate items using ALS
  4. train_ranker: Train and save ranking model
  
- Added new dependencies:
  - implicit for ALS implementation
  - catboost for ranking model
  - scikit-learn for data splitting
  - scipy for sparse matrix operations

## [2025-03-07] - Model Training Pipeline Enhancements

### Added
- Enhanced ranking model with Optuna for Bayesian optimization
- Added conformal prediction support using MAPIE
- Improved model reliability with uncertainty quantification
- Updated dependencies in requirements.txt and Dockerfile

## [2025-03-07] - Advanced Feature Engineering

### Added
- Advanced feature engineering module with sophisticated e-commerce features:
  - Temporal features with multi-window analysis (1/7/30 days)
  - Session-based features with configurable timeout
  - Price-based features with category-relative analysis
  - Category graph analysis using NetworkX
  - Sequence analysis with transition probabilities
  - Time-decay interaction scoring

### Enhanced
- DataProcessor with improved data cleaning:
  - Proper type conversion for IDs
  - Numeric column handling
  - Enhanced conversion rate calculations
  - Integration with advanced features

### Dependencies
- Added NetworkX for category graph analysis
- Added tqdm for progress tracking

## [2025-03-07] - Real-time Prediction Service

### Added
- FastAPI service for real-time recommendations:
  - Real-time and batch recommendation endpoints
  - Event ingestion API
  - Health check and metrics endpoints
  - Confidence interval support from MAPIE
  - Background feature updates

- Prediction Service Features:
  - Feature caching with configurable update intervals
  - Asynchronous feature computation
  - Model version management
  - Combined ALS and CatBoost predictions
  - Real-time feature engineering

### Dependencies
- Added FastAPI and uvicorn for API service
- Added pydantic for data validation
- Added prometheus-client for metrics

### Infrastructure
- Added Dockerfile for prediction service
- Configured environment variables for deployment
- Added cache and model volume mounts

## [2025-03-07] - AWS EKS Deployment

### Added
- Kubernetes deployment configuration:
  - 3 replicas with rolling updates
  - Resource requests and limits
  - Health checks and readiness probes
  - Horizontal Pod Autoscaler (3-10 pods)
  - EFS storage for models and cache

- AWS Infrastructure:
  - EKS cluster integration
  - EFS for shared model storage
  - LoadBalancer service type
  - Prometheus metrics support

### Enhanced
- FastAPI service improvements:
  - OpenAPI documentation
  - Error handling and validation
  - Service metrics and health checks
  - Type hints and response models

### Dependencies
- Updated requirements.txt with exact versions
- Added prometheus-client for metrics

### Next Steps
- Set up AWS EFS storage class
- Configure AWS Load Balancer Controller
- Implement CI/CD pipeline
- Add monitoring with Prometheus/Grafana

## [2025-03-07] - Model Monitoring Implementation

### Added
- Model monitoring service:
  - Real-time performance tracking
  - Data drift detection using KS test
  - Automated retraining triggers
  - S3 integration for metrics storage

- Monitoring components:
  - MetricsCollector for data collection
  - MonitoringService for metrics processing
  - FastAPI endpoints for monitoring
  - Kubernetes deployment configs

### Enhanced
- Project infrastructure:
  - EFS storage for metrics persistence
  - Prometheus metrics integration
  - Service health checks
  - Background tasks for metrics collection

### Dependencies
- Added scipy for statistical tests
- Added boto3 for AWS integration

### Next Steps
- Implement CI/CD pipeline
- Set up A/B testing framework
- Configure Prometheus/Grafana dashboards
- Complete AWS infrastructure setup
